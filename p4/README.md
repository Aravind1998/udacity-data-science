## Identifying Fraud from Enron Emails and Financial Data
by Allan Reyes, in fulfillment of Udacity's [Data Analyst Nanodegree](https://www.udacity.com/course/nd002), Project 4

### About

In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, there was a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for to executives.

Utilizing `scikit-learn` and machine learning methodologies, I built a "person of interest" (POI) identifier to detect and predict culpable persons, using features from financial data, email data, and labeled data--POIs who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity.

### Resources and References

- [Introduction to Machine Learning (Udacity)](https://www.udacity.com/course/viewer#!/c-ud120-nd)
- [Machine Learning (Stanford/Coursera)](https://www.coursera.org/course/ml)
- [scikit-learn Documentation](http://scikit-learn.org/stable/documentation.html)

### Short Questions

> Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?

`NotYetImplemented`

> What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that doesn't come ready-made in the dataset--explain what feature you tried to make, and the rationale behind it. If you used an algorithm like a decision tree, please also give the feature importances of the features that you use.

`NotYetImplemented`

> What algorithm did you end up using? What other one(s) did you try?

`NotYetImplemented`

> What is validation, and what's a classic mistake you can make if you do it wrong? How did you validate your analysis?

`NotYetImplemented`

> Give at least 2 evaluation metrics, and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm's performance.

`NotYetImplemented`

### Files
- `poi_id.py`: POI identifier
- `final_project_dataset.pkl`: dataset for the project
- `tester.py`: Udacity-provided file, produces relevant submission files
- `my_dataset.pkl`: generated dataset
- `my_classifier.pkl`: generated classifier
- `my_feature_list.pkl`: generated feature list
- `emails_by_address/`: contains messages to or from a particular email address
